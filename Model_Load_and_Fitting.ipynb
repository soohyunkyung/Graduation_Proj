import os
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetV2B0
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from google.colab import drive
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')

# Google Drive ë§ˆìš´íŠ¸
drive.mount('/content/drive')

# ì„¤ì •
IMG_SIZE = (384, 384)
MODEL_NAME = 'EfficientNetV2B0'
DRIVE_FOLDER = '/content/drive/MyDrive/dog_breed_model'
MODEL_PATH = os.path.join(DRIVE_FOLDER, f'{MODEL_NAME}_dog_breed.keras')
CHECKPOINT_PATH = os.path.join(DRIVE_FOLDER, f'{MODEL_NAME}_checkpoint.keras')

# í´ë” ìƒì„±
os.makedirs(DRIVE_FOLDER, exist_ok=True)
print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {DRIVE_FOLDER}\n")

# í´ë˜ìŠ¤ ê°œìˆ˜
num_classes = len(classes)
print(f"ë¶„ë¥˜í•  ê²¬ì¢… ìˆ˜: {num_classes}ê°œ\n")

# ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¶ˆëŸ¬ì˜¤ê¸°
if os.path.exists(MODEL_PATH):
    print(f"âœ… ê¸°ì¡´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤: {MODEL_PATH}")
    model = load_model(MODEL_PATH)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n")

elif os.path.exists(CHECKPOINT_PATH):
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤: {CHECKPOINT_PATH}")
    model = load_model(CHECKPOINT_PATH)
    print("âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ! ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.\n")

else:
    print("âœ… ìƒˆë¡œìš´ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.")

    # Pretrained ëª¨ë¸
    base_model = EfficientNetV2B0(
        include_top=False,
        weights='imagenet',
        input_shape=(*IMG_SIZE, 3)
    )

    # âœ… base_model ì „ì²´ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
    base_model.trainable = True

    # ì„ íƒ: ì´ˆë°˜ ë ˆì´ì–´ëŠ” ê³ ì •í•˜ê³  ë’·ë¶€ë¶„ë§Œ í•™ìŠµ (ë” ì•ˆì •ì )
    # for layer in base_model.layers[:100]:  # ì²˜ìŒ 100ê°œ ë ˆì´ì–´ ê³ ì •
    #     layer.trainable = False

    # Custom classification head
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.3)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    # ëª¨ë¸ êµ¬ì„±
    model = Model(inputs=base_model.input, outputs=outputs)

    # ì»´íŒŒì¼ (ë‚®ì€ learning rateë¡œ ì‹œì‘)
    model.compile(
        optimizer=Adam(learning_rate=1e-4),  # base_model í•™ìŠµ ìœ„í•´ ë‚®ì€ lr
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    print("âœ… ìƒˆ ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
    print(f"âœ… Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}")
    print()


# ì½œë°± ì„¤ì •
checkpoint = ModelCheckpoint(
    filepath=CHECKPOINT_PATH,
    monitor='val_accuracy',
    save_best_only=True,
    save_weights_only=False,
    mode='max',
    verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    patience=15,
    restore_best_weights=True,
    mode='max',
    verbose=1
)

# í•™ìŠµ ì§„í–‰ ìƒí™© ì¶œë ¥ ì½œë°±
class PrintProgress(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print(f"\nğŸ“Š Epoch {epoch+1} ì™„ë£Œ:")
        print(f"  - Train Accuracy: {logs['accuracy']:.4f}")
        print(f"  - Val Accuracy: {logs['val_accuracy']:.4f}")
        print(f"  - Train Loss: {logs['loss']:.4f}")
        print(f"  - Val Loss: {logs['val_loss']:.4f}\n")

print("="*60)
print("ğŸš€ í•™ìŠµ ì‹œì‘")
print("="*60)

# í•™ìŠµ
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=5,
    callbacks=[checkpoint, early_stopping, PrintProgress()],
    verbose=1
)

# ìµœì¢… ëª¨ë¸ ì €ì¥
print("\n" + "="*60)
print("ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
model.save(MODEL_PATH)
print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODEL_PATH}")
print("="*60)
